Logistic regression using stochastic gradient descent
-----------------------------------------------------
(c) Tim Nugent

Compile by running 'make'. Uses std=c++11 - on older compilers you may need to change this to 'std=c++0x' in the Makefile.

Run all tests with 'make test'

Run the train/classify tool as follows:

	./lr_sgd train.dat test.dat

Training and test data should be in svm-light/libsvm format, e.g.

+1 1:4.12069 2:11.3896 3:18.5742 4:2.85764 5:53.4406 
-1 1:3.14565 2:17.4338 3:19.3353 4:2.63431 5:56.4233

Sparse data is OK. Stochastic gradient descent is sensitive to feature scaling, so it is highly recommended that you scale your data e.g. by standardising to Z-scores, or scaling in the range [0,1]. Only 2 data classes are permitted (e.g. 0 and 1, or -1 and 1). Full options are as follows:

	./lr_sgd [options] training_data test_data

	Options:
	-s <int>   Shuffle dataset after each iteration. default 1
	-i <int>   Maximum iterations. default 50000
	-e <float> Convergence rate. default 0.005
	-a <float> Learning rate. default 0.001
	-m <file>  Write weights to model file
	-v         Verbose.

Test example
------------

This uses training/test data from Reuters articles about corporate acquisitions, borrowed from here: http://svmlight.joachims.org/. Use the -v flag to see the prediction results.

./lr_sgd train.dat test.dat
# called with:       ./lr_sgd train.dat test.dat 
# learning rate:     0.001
# convergence rate:  0.005
# max. iterations:   50000
# training data:     train.dat
# test data:         test.dat
# training examples: 2000
# features:          7034
# stochastic gradient descent:
# convergence: 0.0529 iterations: 100
# convergence: 0.0390 iterations: 200
# convergence: 0.0311 iterations: 300
# convergence: 0.0260 iterations: 400
# convergence: 0.0224 iterations: 500
# convergence: 0.0197 iterations: 600
# convergence: 0.0177 iterations: 700
# convergence: 0.0161 iterations: 800
# convergence: 0.0147 iterations: 900
# convergence: 0.0136 iterations: 1000
# convergence: 0.0127 iterations: 1100
# convergence: 0.0119 iterations: 1200
# convergence: 0.0112 iterations: 1300
# convergence: 0.0106 iterations: 1400
# convergence: 0.0100 iterations: 1500
# convergence: 0.0096 iterations: 1600
# convergence: 0.0091 iterations: 1700
# convergence: 0.0087 iterations: 1800
# convergence: 0.0084 iterations: 1900
# convergence: 0.0080 iterations: 2000
# convergence: 0.0077 iterations: 2100
# convergence: 0.0075 iterations: 2200
# convergence: 0.0072 iterations: 2300
# convergence: 0.0070 iterations: 2400
# convergence: 0.0068 iterations: 2500
# convergence: 0.0066 iterations: 2600
# convergence: 0.0064 iterations: 2700
# convergence: 0.0062 iterations: 2800
# convergence: 0.0060 iterations: 2900
# convergence: 0.0059 iterations: 3000
# convergence: 0.0057 iterations: 3100
# convergence: 0.0056 iterations: 3200
# convergence: 0.0054 iterations: 3300
# convergence: 0.0053 iterations: 3400
# convergence: 0.0052 iterations: 3500
# convergence: 0.0051 iterations: 3600
# classifying:
# accuracy: 97.83 % (587/600)







